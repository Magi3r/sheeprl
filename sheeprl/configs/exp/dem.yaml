# @package _global_

defaults:
  - dreamer_v3
  - _self_

## Episodic Memory Stuff ##
episodic_memory:
  trajectory_length: 20
  capacity: 512
  k_neighbors: 10
  uncertainty_threshold: 0.9
  prune_fraction: 0.2
  time_to_live: 100
  rehersal_train_every: 512
  enable_rehersal_training: true
  use_acd: true
  fill_parallel_to_buffer: true
  normalize_kNN: false
  softmax_kNN: false             # apply softmax to z in key

  ### legacy, will remove if nothings breaks ###
  # read_threshold_last_N: 128
  # percentile_treshold: 90
  # write_uncertainty_windows_size: 128 
  # read_std_multiplier: 1.0
  # write_std_multiplier: 1.0
  ###--------------------------------###

  # configure dynamic thresholds
  # moving average for inserting, as we only get one single uncertainty per real env step
  # exponential moving average for reading, as we get multiple uncertainties per dreaming step and not only a single interaction per step
  # so would be more complicated to track
  read_exp_mov_avg_alpha: 0.99    # moving average alpha for read threshold (inside dreaming [when to read from em])
  write_window_size_N: 128        # window size N for write threshold moving average (env interactions [when to write to em])

  read_std_multiplier_start: 0.0  # initial multiplier for read threshold std 
  read_std_multiplier_max: 1.0    # final multiplier for read threshold std
  read_std_inc_addend: 0.0001      # exponential increase factor for read threshold std (not exponentail anymore, but additive)
  read_std_inc_start_at_ep: 000   # in which episode to start increasing read std multiplier
  
  write_std_multiplier_start: 0.0 # initial multiplier for write threshold std
  write_std_multiplier_max: 1.0   # final multiplier for write threshold std
  write_std_inc_addend: 0.0001     # exponential increase factor for write threshold std (not exponentail anymore, but additive)
  write_std_inc_start_at_ep: 000  # in which to start increasing write std multiplier
  # ---------------------------------###

# Algorithm
algo:
  replay_ratio: 1

  # per_rank_batch_size: 16
  # per_rank_sequence_length: 64
  # cnn_keys:
  #   encoder: [rgb]
  #   decoder: [rgb]

# Checkpoint
# checkpoint:
#   every: 100000

# Buffer
# buffer:
#   size: 1000000
#   checkpoint: True

# Distribution
# distribution:
#   type: "auto"

metric:
  log_every: 1000
  # aggregator:
  #   metrics:
  #     Loss/world_model_loss:
  #       _target_: torchmetrics.MeanMetric
  #       sync_on_compute: ${metric.sync_on_compute}
  #     Loss/value_loss:
  #       _target_: torchmetrics.MeanMetric
  #       sync_on_compute: ${metric.sync_on_compute}
  #     Loss/policy_loss:
  #       _target_: torchmetrics.MeanMetric
  #       sync_on_compute: ${metric.sync_on_compute}
  #     Loss/observation_loss:
  #       _target_: torchmetrics.MeanMetric
  #       sync_on_compute: ${metric.sync_on_compute}
  #     Loss/reward_loss:
  #       _target_: torchmetrics.MeanMetric
  #       sync_on_compute: ${metric.sync_on_compute}
  #     Loss/state_loss:
  #       _target_: torchmetrics.MeanMetric
  #       sync_on_compute: ${metric.sync_on_compute}
  #     Loss/continue_loss:
  #       _target_: torchmetrics.MeanMetric
  #       sync_on_compute: ${metric.sync_on_compute}
  #     State/kl:
  #       _target_: torchmetrics.MeanMetric
  #       sync_on_compute: ${metric.sync_on_compute}
  #     State/post_entropy:
  #       _target_: torchmetrics.MeanMetric
  #       sync_on_compute: ${metric.sync_on_compute}
  #     State/prior_entropy:
  #       _target_: torchmetrics.MeanMetric
  #       sync_on_compute: ${metric.sync_on_compute}
  #     Grads/world_model:
  #       _target_: torchmetrics.MeanMetric
  #       sync_on_compute: ${metric.sync_on_compute}
  #     Grads/actor:
  #       _target_: torchmetrics.MeanMetric
  #       sync_on_compute: ${metric.sync_on_compute}
  #     Grads/critic:
  #       _target_: torchmetrics.MeanMetric
  #       sync_on_compute: ${metric.sync_on_compute}
